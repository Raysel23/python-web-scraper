# python-web-scraper
# Hybrid Web Scraper Framework (Requests + Selenium)

## ğŸ“Œ Project Overview
This project is a **hybrid Python web scraping framework** that automatically determines the best scraping strategy for a given URL.

It can:
- Detect if a website exposes an **API (JSON response)**
- Use **Requests** for API-based scraping
- Fall back to **Selenium** for dynamic or JavaScript-rendered pages
- Handle scrolling, pagination, retries, logging, and structured data storage

This project is built as a **portfolio-ready scraper**, following real-world scraping patterns and best practices.

Target site used for testing: **Books to Scrape** (public demo website).

---

## ğŸ¯ Features
- Automatic API detection
- Requests-based scraping for APIs
- Selenium-based scraping for dynamic pages
- Infinite scroll handling
- Pagination via â€œNextâ€ button
- Retry logic for failed requests
- Duplicate page detection
- Centralized logging (file + console)
- Save scraped data to JSON and CSV
- Modular, reusable architecture

---

## ğŸ§  Scraping Logic Flow
Start
â””â”€â”€ Check if URL returns JSON
â”œâ”€â”€ Yes â†’ Use Requests â†’ Save JSON
â””â”€â”€ No â†’ Use Selenium
â”œâ”€â”€ Scroll page
â”œâ”€â”€ Handle pagination
â”œâ”€â”€ Collect HTML pages
â”œâ”€â”€ Parse data
â””â”€â”€ Save JSON & CSV

---

## ğŸ›  Tech Stack
- Python 3
- Requests
- Selenium (Chrome WebDriver)
- BeautifulSoup (bs4)
- Logging
- CSV / JSON

---

## ğŸ“‚ Project Structure
project/
â”‚
â”œâ”€â”€ base_scrap.py # Requests-based scraper
â”œâ”€â”€ base_selenium.py # Selenium scraper (scroll & pagination)
â”œâ”€â”€ api_check.py # API detection logic
â”œâ”€â”€ parse.py # HTML parsing logic
â”œâ”€â”€ save.py # JSON & CSV saving utilities
â”œâ”€â”€ log.py # Logging configuration
â”œâ”€â”€ main.py # Main entry point
â”‚
â”œâ”€â”€ Results/
â”‚ â”œâ”€â”€ selenium_products.json
â”‚ â””â”€â”€ selenium_products.csv
â”‚
â””â”€â”€ scrap.log # Log file


---

## âš™ï¸ How It Works

### 1ï¸âƒ£ API Detection
The scraper first checks whether the URL returns a valid JSON response.

If JSON data is detected, the scraper switches to **Requests** for faster and cleaner data extraction.

---

### 2ï¸âƒ£ Requests-Based Scraping
- Used only when an API is detected
- Parses JSON directly
- Saves output as a `.json` file

---

### 3ï¸âƒ£ Selenium Scraping
Used when no API is detected.

Features:
- Headless Chrome browser
- Safe page loading with retries
- Infinite scrolling support
- Pagination handling using a â€œNextâ€ button
- Duplicate HTML page detection

---

### 4ï¸âƒ£ Data Parsing
HTML pages are parsed using **BeautifulSoup** to extract:
- Book title
- Price
- Availability
- Product link

---

### 5ï¸âƒ£ Data Storage
Scraped data is saved as:
- **JSON** (structured storage)
- **CSV** (analysis-ready format)

All files are stored inside the `Results/` directory.

---

## â–¶ï¸ How to Run

### Requirements
- Python 3.9+
- Google Chrome
- ChromeDriver available in system PATH

### Install dependencies


pip install requests selenium beautifulsoup4


âš ï¸ Disclaimer

This project is for educational and portfolio purposes

Target site is a public scraping demo

Always respect robots.txt and site terms when scraping real websites
